{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5510dc03",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "# Unsolicited Message Classification\n",
    "\n",
    "In this project I have used text classification to determined whether the messages is unsolicited messages or not. I have used NLP methods to prepared and clean text data (tokenization, remove stop words, stemming) and different machine learning algorithms to get more accurate predictions. The following classification algorithms have been used: Logistic Regression, Naive Bayes, Support Vector Machine (SVM), Random Forest, Stochastic Gradient Descent and Gradient Boosting.\n",
    "\n",
    "### Dataset\n",
    "The dataset comes from SMS Unsolicited Message Collection that can be find at Kaggle.\n",
    "\n",
    "This SMS Unsolicited Message Collection is a set of SMS tagged messages that have been collected for SMS Unsolicited Message research. It comprises one set of SMS messages in English of 5,574 messages, which is tagged acording being ham (legitimate) or unsolicited messages. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae347ab6",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "## Import libriaries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b5b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae491b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = pd.read_csv(r'C:\\Python Scripts\\Datasets\\msg.csv',  encoding='latin-1')\n",
    "msg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c159e19",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "First observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfcb54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72408e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51cc56",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "There are 86 961 words in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd152ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(msg['v2'].apply(lambda x: len(x.split(' '))).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f2b130",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "### Data cleaning\n",
    "\n",
    "Remove unnecessary variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fdd250",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618d02f",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "Rename columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9a69c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg.rename(columns={'v1': 'Class', 'v2': 'Text'}, inplace=True)\n",
    "msg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf183e17",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "Changing column \"Class\" to 0 and 1:\n",
    "\n",
    "- unsolicited messages = 1\n",
    "- ham = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aef996",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg['Class'] = msg['Class'].map({'normal':0, 'msg':1})\n",
    "msg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245689e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg['Text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c038b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg['Text'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb6e9ba",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "## Data analysis\n",
    "\n",
    "Checking proportion 'Class' variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d583f932",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Class',data=msg)\n",
    "plt.xlabel('Class')\n",
    "plt.title('Number of normal and msg messages');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8262ff3",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "The target class variable is imbalanced, where \"ham\" values are more dominating than \"unsolicited messages\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e7d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60aa60c",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "Length of text messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa4b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg['length'] = msg.Text.apply(len)\n",
    "msg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de0fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "msg[msg.Class == 0].length.plot(bins=35, kind='hist', color='blue', label='Ham', alpha=0.6)\n",
    "msg[msg.Class == 1].length.plot(kind='hist', color='red', label='Msg', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Message Length\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6de5b",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "### Text Pre-processing\n",
    "\n",
    "In the next step I clean text, remove stop words and apply stemming operation for each line of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8133397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "print(stop_words[::10])\n",
    "\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d96b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(words):\n",
    "    \"\"\"The function to clean text\"\"\"\n",
    "    words = re.sub(\"[^a-zA-Z]\",\" \", words)\n",
    "    text = words.lower().split()                   \n",
    "    return \" \".join(text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"The function to removing stopwords\"\"\"\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in stop_words]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def stemmer(stem_text):\n",
    "    \"\"\"The function to apply stemming\"\"\"\n",
    "    stem_text = [porter.stem(word) for word in stem_text.split()]\n",
    "    return \" \".join(stem_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe72f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg['Text'] = msg['Text'].apply(clean_text)\n",
    "msg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c088071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg['Text'] = msg['Text'].apply(remove_stopwords)\n",
    "msg['Text'] = msg['Text'].apply(stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad89d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(msg['Text'].apply(lambda x: len(x.split(' '))).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c017b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save clean data\n",
    "msg.to_csv('C:\\\\Python Scripts\\\\NLP_projekty\\\\msg_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab742b",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "After text cleaning and removing stop words there are only 49 940 words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c8b0fc",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "### Vectorization\n",
    "\n",
    "To run machine learning algorithms need to convert text files into numerical feature vectors. I use bag of words model for the analysis. \n",
    "\n",
    "First I splitting the data into X and y values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = msg['Text']\n",
    "y = msg['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ed187",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "Creating a numerical feature vector for each document: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X)\n",
    "\n",
    "X_vec = vect.transform(X)\n",
    "\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c43feb",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "Splitting the data into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732fb159",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state = 0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6448bd5",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "**SMOTE**\n",
    "\n",
    "The target class variable is imbalanced, \"ham\" values are more dominating than \"unsolicited messages\". The simplest way to improve imbalanced dataset is balancing them by oversampling instances of the minority class or undersampling instances of the majority class. I will use one of the advanced techniques like the SMOTE method (Synthetic Minority Over-sampling Technique) to balancing classes.\n",
    "\n",
    "SMOTE technique  is one of the most commonly used oversampling methods to solve the imbalance problem. It goal is to balance class distribution by randomly increasing  minority class examples by replicating them.  \n",
    "\n",
    "To apply SMOTE method I use imbalanced-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e20f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "X_train_sm,y_train_sm = smote.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e6a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_sm.shape)\n",
    "print(y_train_sm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afec86f",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "## Models\n",
    "\n",
    "I use the following classification models:\n",
    "\n",
    "- Logistic Regression,\n",
    "- Naive Bayes Classifier,\n",
    "- Random Forest Classifier,\n",
    "- Gradient Boosting,\n",
    "- SVM (Support Vector Machine),\n",
    "- Stochastic Gradient Descent.\n",
    "\n",
    "To make the vectorizer => transformer => classifier easier to work with I use Pipeline class in Scilkit-Learn.\n",
    "\n",
    "\n",
    "**Logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba18feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = Pipeline([('tfidf', TfidfTransformer()),\n",
    "                   ('model',LogisticRegression()),\n",
    "                   ])\n",
    "\n",
    "model_lr.fit(X_train_sm,y_train_sm)\n",
    "\n",
    "ytest = np.array(y_test)\n",
    "pred_y = model_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d747ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy %s' % accuracy_score(pred_y, y_test))\n",
    "print(classification_report(ytest, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c490f7bf",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "**Naive Bayes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe50f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb = Pipeline([('tfidf', TfidfTransformer()),\n",
    "                   ('model',MultinomialNB()),\n",
    "                   ])\n",
    "\n",
    "model_nb.fit(X_train_sm,y_train_sm)\n",
    "\n",
    "ytest = np.array(y_test)\n",
    "pred = model_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337aabe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy %s' % accuracy_score(pred, y_test))\n",
    "print(classification_report(ytest, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce9693",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "**Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca68de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = Pipeline([('tfidf', TfidfTransformer()),\n",
    "                   ('model',RandomForestClassifier(n_estimators=50)),\n",
    "                   ])\n",
    "\n",
    "model_rf.fit(X_train_sm,y_train_sm)\n",
    "\n",
    "ytest = np.array(y_test)\n",
    "preds = model_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe6e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy %s' % accuracy_score(preds, y_test))\n",
    "print(classification_report(ytest, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dcf233",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "**Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7621dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gb = Pipeline([('tfidf', TfidfTransformer()),\n",
    "                    ('model', GradientBoostingClassifier(random_state=100, n_estimators=150,min_samples_split=100, max_depth=6)),\n",
    "                    ])\n",
    "\n",
    "model_gb.fit(X_train_sm,y_train_sm)\n",
    "\n",
    "ytest = np.array(y_test)\n",
    "y_pred = model_gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1724ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(ytest, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982cf6ba",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "**Support Vector Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1e2b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svc = Pipeline([('tfidf', TfidfTransformer()),\n",
    "                     ('model',LinearSVC()),\n",
    "                     ])\n",
    "\n",
    "model_svc.fit(X_train_sm,y_train_sm)\n",
    "\n",
    "ytest = np.array(y_test)\n",
    "predict = model_svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21fff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy %s' % accuracy_score(predict, y_test))\n",
    "print(classification_report(ytest, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be508a0",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "**Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a24dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sg = Pipeline([('tfidf', TfidfTransformer()),\n",
    "                     ('model',SGDClassifier()),\n",
    "                     ])\n",
    "\n",
    "model_sg.fit(X_train_sm,y_train_sm)\n",
    "\n",
    "ytest = np.array(y_test)\n",
    "predicted = model_sg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd9f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy %s' % accuracy_score(predicted, y_test))\n",
    "print(classification_report(ytest, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d108aa",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "### Best model\n",
    "\n",
    "I tested six different models and now I check which one is the best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def1c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_acc = accuracy_score(pred_y, y_test)\n",
    "nb_acc = accuracy_score(pred, y_test)\n",
    "rf_acc = accuracy_score(preds, y_test)\n",
    "gb_acc = accuracy_score(y_pred, y_test)\n",
    "svm_acc = accuracy_score(predict, y_test)\n",
    "sg_acc = accuracy_score(predicted, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b226ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.DataFrame({\n",
    "                      'Model': ['Logistic Regression', 'Naive Bayes', 'Random Forest', 'Gradient Boosting', 'SVM', 'SGD'],\n",
    "                      'Score': [log_acc, nb_acc, rf_acc, gb_acc, svm_acc, sg_acc]})\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c496c8a4",
   "metadata": {},
   "source": [
    "### Reworked Cell\n",
    "## Summary\n",
    "\n",
    "This project was aimed to text classification to determined whether the messages is unsolicited messages or not. I have started with the dcleaning and text mining, which cover change text into tokens, remove punctuation, stop words and normalization them by stemming. Following I have used bag of words model to convert the text into numerical feature vectors. Finally I have started training six different classification models and we got the best accuracy of 0.97 for Naive Bayes method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a52e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
